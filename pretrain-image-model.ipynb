{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up Environment\n",
    "\n",
    "First, ensure your environment is set up with the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c pytorch-nightly torchvision\n",
    "!pip install img2vec_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Load and Process Video Frames\n",
    "\n",
    "We'll write a Python script to load videos, extract frames at a specified frame rate, and prepare the dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image\n",
    "\n",
    "def load_video_frames(video_path, fps=8):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip = max(1, round(frame_rate / fps))\n",
    "    \n",
    "    success, frame = video.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        if count % frame_skip == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame_rgb))\n",
    "        success, frame = video.read()\n",
    "        count += 1\n",
    "    \n",
    "    video.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Frame Embedding\n",
    "\n",
    "Use the `Img2Vec` library to replace images with their embedded representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_embeddings(frames, img2vec):\n",
    "    return [img2vec.get_vec(frame, tensor=True) for frame in frames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare Training Data\n",
    "\n",
    "Generate input-target pairs from frame embeddings, initializing with null frames for indices < 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(embedded_frames):\n",
    "    X, y = [], []\n",
    "    null_frame = torch.zeros(embedded_frames[0].shape)  # Adjust shape accordingly\n",
    "    \n",
    "    for i in range(len(embedded_frames)):\n",
    "        X.append([\n",
    "            null_frame if i - 3 < 0 else embedded_frames[i - 3],\n",
    "            null_frame if i - 2 < 0 else embedded_frames[i - 2],\n",
    "            null_frame if i - 1 < 0 else embedded_frames[i - 1],\n",
    "        ])\n",
    "        y.append(embedded_frames[i])\n",
    "        \n",
    "    return torch.stack([torch.stack(x) for x in X]), torch.stack(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader:\n",
    "\n",
    " create a custom dataset loader that iterates over all MP4 files in a given directory, loads the videos, processes them into frames, and applies the embedding model to each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, video_paths, img2vec, transform=None, fps=8):\n",
    "        self.video_paths = video_paths\n",
    "        self.img2vec = img2vec\n",
    "        self.transform = transform\n",
    "        self.fps = fps\n",
    "        self.dataset = self.load_and_process_videos()\n",
    "\n",
    "    def load_and_process_videos(self):\n",
    "        dataset = []\n",
    "        for video_path in self.video_paths:\n",
    "            frames = load_video_frames(video_path, self.fps)\n",
    "            embedded_frames = get_frame_embeddings(frames, self.img2vec)\n",
    "            dataset.extend(prepare_data(embedded_frames))\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "def load_video_paths(training_path):\n",
    "    video_paths = [os.path.join(training_path, f) for f in os.listdir(training_path) if f.endswith('.mp4')]\n",
    "    return video_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Instantiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Img2Vec with CUDA if available\n",
    "img2vec = Img2Vec(cuda=torch.cuda.is_available(), model=\"efficientnet_b0\")\n",
    "\n",
    "training_path = 'videos/'\n",
    "video_paths = load_video_paths(training_path)\n",
    "\n",
    "# Instantiate your dataset\n",
    "dataset = VideoFrameDataset(video_paths, img2vec, fps=8)\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Neural Network Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
